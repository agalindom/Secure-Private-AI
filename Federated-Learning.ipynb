{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandrogalindo/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dists\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import syft\n",
    "\n",
    "random.seed(123)\n",
    "np.random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "hook = syft.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<VirtualWorker id:Worker 0 #tensors:0>, <VirtualWorker id:Worker 1 #tensors:0>, <VirtualWorker id:Worker 2 #tensors:0>, <VirtualWorker id:Worker 3 #tensors:0>, <VirtualWorker id:Worker 4 #tensors:0>, <VirtualWorker id:Worker 5 #tensors:0>, <VirtualWorker id:Worker 6 #tensors:0>, <VirtualWorker id:Worker 7 #tensors:0>, <VirtualWorker id:Worker 8 #tensors:0>, <VirtualWorker id:Worker 9 #tensors:0>, <VirtualWorker id:Worker 10 #tensors:0>, <VirtualWorker id:Worker 11 #tensors:0>, <VirtualWorker id:Worker 12 #tensors:0>, <VirtualWorker id:Worker 13 #tensors:0>, <VirtualWorker id:Worker 14 #tensors:0>, <VirtualWorker id:Worker 15 #tensors:0>, <VirtualWorker id:Worker 16 #tensors:0>, <VirtualWorker id:Worker 17 #tensors:0>, <VirtualWorker id:Worker 18 #tensors:0>, <VirtualWorker id:Worker 19 #tensors:0>, <VirtualWorker id:Worker 20 #tensors:0>, <VirtualWorker id:Worker 21 #tensors:0>, <VirtualWorker id:Worker 22 #tensors:0>, <VirtualWorker id:Worker 23 #tensors:0>]\n"
     ]
    }
   ],
   "source": [
    "number_workers = 24\n",
    "\n",
    "workers = [syft.VirtualWorker(hook, id=\"Worker {}\".format(i)) for i in range(number_workers)]\n",
    "print(workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.MNIST(root='../data', train=True, download=True, transform= transforms.ToTensor())\n",
    "testset  = datasets.MNIST(root='../data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "print(len(trainset))\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset\n",
      "    Distributed accross: Worker 0, Worker 1, Worker 2, Worker 3, Worker 4, Worker 5, Worker 6, Worker 7, Worker 8, Worker 9, Worker 10, Worker 11, Worker 12, Worker 13, Worker 14, Worker 15, Worker 16, Worker 17, Worker 18, Worker 19, Worker 20, Worker 21, Worker 22, Worker 23\n",
      "    Number of datapoints: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "federated_trainset = trainset.federate(workers)\n",
    "print(federated_trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNET(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NNET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 14, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 14, out_channels = 28, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.conv3 = nn.Conv2d(in_channels = 28, out_channels = 42, kernel_size = 5, stride = 1, padding = 2)\n",
    "        self.fc1 = nn.Linear(in_features = 6*6*42, out_features = 500)\n",
    "        self.fc2 = nn.Linear(in_features = 500, out_features = 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 1)\n",
    "        x = x.view(-1, 6*6*42)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNET()\n",
    "epochs = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Training Loss: 2.304\tTraining Accuracy: 0.087\n",
      "Epoch: 1\t Training Loss: 2.279\tTraining Accuracy: 0.134\n",
      "Epoch: 2\t Training Loss: 2.255\tTraining Accuracy: 0.133\n",
      "Epoch: 3\t Training Loss: 2.142\tTraining Accuracy: 0.350\n",
      "Epoch: 4\t Training Loss: 1.895\tTraining Accuracy: 0.454\n",
      "Epoch: 5\t Training Loss: 1.618\tTraining Accuracy: 0.470\n",
      "Epoch: 6\t Training Loss: 1.808\tTraining Accuracy: 0.322\n",
      "Epoch: 7\t Training Loss: 1.398\tTraining Accuracy: 0.522\n",
      "Epoch: 8\t Training Loss: 1.425\tTraining Accuracy: 0.525\n",
      "Epoch: 9\t Training Loss: 1.144\tTraining Accuracy: 0.605\n",
      "Epoch: 10\t Training Loss: 1.201\tTraining Accuracy: 0.629\n",
      "Epoch: 11\t Training Loss: 1.033\tTraining Accuracy: 0.725\n",
      "Epoch: 12\t Training Loss: 0.852\tTraining Accuracy: 0.737\n",
      "Epoch: 13\t Training Loss: 0.805\tTraining Accuracy: 0.738\n",
      "Epoch: 14\t Training Loss: 0.644\tTraining Accuracy: 0.781\n",
      "Epoch: 15\t Training Loss: 0.613\tTraining Accuracy: 0.806\n",
      "Epoch: 16\t Training Loss: 0.606\tTraining Accuracy: 0.780\n",
      "Epoch: 17\t Training Loss: 0.660\tTraining Accuracy: 0.794\n",
      "Epoch: 18\t Training Loss: 0.557\tTraining Accuracy: 0.798\n",
      "Epoch: 19\t Training Loss: 0.539\tTraining Accuracy: 0.828\n",
      "Epoch: 20\t Training Loss: 0.455\tTraining Accuracy: 0.867\n",
      "Epoch: 21\t Training Loss: 0.474\tTraining Accuracy: 0.858\n",
      "Epoch: 22\t Training Loss: 0.416\tTraining Accuracy: 0.853\n",
      "Epoch: 23\t Training Loss: 0.416\tTraining Accuracy: 0.865\n",
      "Epoch: 24\t Training Loss: 0.358\tTraining Accuracy: 0.874\n",
      "Epoch: 25\t Training Loss: 0.346\tTraining Accuracy: 0.896\n",
      "Epoch: 26\t Training Loss: 0.344\tTraining Accuracy: 0.887\n",
      "Epoch: 27\t Training Loss: 0.337\tTraining Accuracy: 0.892\n",
      "Epoch: 28\t Training Loss: 0.317\tTraining Accuracy: 0.908\n",
      "Epoch: 29\t Training Loss: 0.288\tTraining Accuracy: 0.922\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    model = model.send(syft.local_worker)\n",
    "\n",
    "    batch_idx = torch.tensor(0.).send(syft.local_worker)\n",
    "    training_loss   = torch.tensor(0.).send(syft.local_worker)\n",
    "    correct_preds  = torch.tensor(0.).send(syft.local_worker)\n",
    "    for i, worker in enumerate(workers): \n",
    "        model.move(worker)\n",
    "        batch_idx.move(worker)\n",
    "        training_loss.move(worker)\n",
    "        correct_preds.move(worker)\n",
    "        dataset = federated_trainset.datasets[worker.id]\n",
    "        dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        img,target = next(iter(dataloader))\n",
    "        batch_idx.add_(img.shape[0]) \n",
    "        output = model(img)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        training_loss.add_(loss.data)\n",
    "        correct_preds.add_(torch.sum(torch.eq(output.data.argmax(dim=1), target)))\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.grad.div_(batch_idx)\n",
    "\n",
    "    model = model.get() \n",
    "\n",
    "    Loss     = training_loss.div_(batch_idx).get().item()\n",
    "    Accuracy = correct_preds.div_(batch_idx).get().item()\n",
    "\n",
    "    \n",
    "    print('Epoch: {}\\t Training Loss: {:.3f}\\tTraining Accuracy: {:.3f}'.format(epoch, Loss, Accuracy))\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.251\tTest Accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "testloader = data.DataLoader(testset, batch_size=1000)\n",
    "test_loss = 0\n",
    "batch_idx = 0\n",
    "correct_preds  = 0\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, labels) in enumerate(testloader, 1):\n",
    "        batch_idx += imgs.size(0)\n",
    "\n",
    "        preds = model(imgs)\n",
    "\n",
    "        test_loss += criterion(preds, labels).item()\n",
    "        correct_preds += (preds.argmax(dim=1) == labels).sum().item()\n",
    "    \n",
    "    Loss = test_loss/batch_idx\n",
    "    Accuracy = correct_preds/batch_idx\n",
    "print('Test Loss: {:.3f}\\tTest Accuracy: {:.3f}'.format(Loss, Accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
